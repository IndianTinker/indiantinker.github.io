---
layout: post
title: How do I look honey?
image: https://i.imgur.com/667g4d4.png
location:  IDC IIT Bombay India
tags:
- design
- select

field: design fiction<br>creative technology
---

> The project explores the social value of human beauty in a possible future (among many futures). In this imagined world, the society does not possess natural beauty due various genetic mutations triggered by solar flares. They live in a time where, beauty is simulated by technology. I explore many interactions in this future when a naturally occouring thing like human beauty, becomes manipulated and managed by technology, the kind of control, regulatory, market and misuse dialogues it generates. I presented the project as a solo act by developing prototype interactions that exist in the future. The vehicle to deliver the story was a log generated by a person who made such a technology. If someone knows about the *Judgement of Thamus* from *Plato's Phaedrus* or/and *Neil Postman's Technopoly*, a similar rhetoric was used but inside a story. I discuss things like digital cosmetics (now filters), beauty as a service, and liquid personalities. This project was done during Masters at IDC during the course Trends in Interaction Design by Prof. Venkat and Jayesh.


My project &#8212; **How do I look, Honey?**, explores and challenges the fundamental human bias of visual beauty. What happens when technology can alter that? How will you define it? Will it become an asset that will be controlled by the rich?

The story is set in a time when people are no longer beautiful. A catastrophic solar flare makes everyone ugly. People in these days use eyeAL devices which augment their vision to show them a digital persona of the real person. This digital persona can be stylized with cosmetics as in the past, but today these cosmetics are digital.People spend money to look good. They can things like fairness recharges or postpaid rental plans on beauty products. They can ensure they never age visually by paying for beauty insurance.

In this future, the advertisements have taken a huge leap. The ads at places recognize your intent to buy and pitch your products accordingly. People are known to rent their bodies to augment advertisements. Everything you see is personalized. You can also have digital furniture whose texture and color you can change without changing the sofa ever. Same goes for a lot of personalized things like cars. How will it be to look at such an augmented world? 

The protagonist in my story is a person who is a part of the team that designed this device. He has been invited by his current self to explain the happenings of his day. In his days, the designers are not the one who design everything. Incremental designs are made by machines entirely and only when machines canâ€™t rationalize the decision a human design debugger is looped in. 

There are personality hackers that can digitally transplant your personality. Sometimes even the dead are known to commit crimes using transplanted personas. The government can block some people from being viewed by others as a formal of penalization.

How beautiful is such a world?

<h4>Prototype</h4>

In the prototype, I acted out as a hologram from the future world, explaining the happening of the future using the presentation. The audiences, were asked to interact with the hologram (i.e me) using an AR app I made.

<a href="http://rohitg.in/designfiction/index.html">![Play](https://i.imgur.com/667g4d4.png)</a>

The prototype blog can be seen  [here](http://rohitg.in/designfiction/index.html).

One of the videos shows the scenario of video chatting with a person, who has not paid his rent. 

<iframe width="560" height="315" src="https://www.youtube.com/embed/MitBfqTiFBQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

The course demonstration is hosted on this [URL](https://rohit7gupta.github.io/explore/objects.html). To view the demonstration the compatible markers are uploaded [here](http://rohitg.in/designfiction/markers.pdf).


The technologies I used were:

- AR.js (which uses AFrame and Three.js)
- Processing and OpenCV for Blocked and Face Tracking demos



Cheers,

Rohit
